{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9CC0NZOAZ6+V+WmJEfe1w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zinebzannouti/DL-CarClassificationWithFlask/blob/main/Project-Solution-Part/4_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine Tuning**"
      ],
      "metadata": {
        "id": "mNYBg5Qb9hXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Once your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate.\n",
        "\n",
        "- This is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting -- keep that in mind.\n",
        "\n",
        "- It is critical to only do this step after the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features.\n",
        "\n",
        "- It's also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to readapt the pretrained weights in an incremental way.\n",
        "\n",
        "- This is how to implement fine-tuning of the whole base model:"
      ],
      "metadata": {
        "id": "VvMSmi2U-aHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unfreeze the base model**"
      ],
      "metadata": {
        "id": "97Sh_5a4_PyP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfkrVkWa9BIn"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the base model\n",
        "base_model.trainable = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TO DO**"
      ],
      "metadata": {
        "id": "BsilyXsYtrWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compiling the model with a very low learning rate**"
      ],
      "metadata": {
        "id": "t6NsGchU_WU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It's important to recompile your model after you make any changes\n",
        "# to the `trainable` attribute of any inner layer, so that your changes\n",
        "# are take into account\n",
        "model.compile(optimizer=Adam(1e-5),  # Very low learning rate\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "GFohtlJl-mNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retraining The model**"
      ],
      "metadata": {
        "id": "XtKAol8y_egl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train end-to-end\n",
        "history = model.fit_generator(generator=train_generator,\n",
        "                    steps_per_epoch=train_generator.samples // batch_size + 1 ,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=validation_generator.samples // batch_size + 1,\n",
        "                    epochs=10,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "XB1igpq_-ro5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save The Model**"
      ],
      "metadata": {
        "id": "1EAC3bl2_iOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model\n",
        "model.save('model.h5')"
      ],
      "metadata": {
        "id": "VD0ph4rK_mWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}